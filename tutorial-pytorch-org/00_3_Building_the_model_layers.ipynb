{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_3_Building_the_model_layers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8lD8BTSmCj5mCwpsZeAJj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desaiankitb/pytorch-basics/blob/main/tutorial-pytorch-org/00_3_Building_the_model_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e-jAg5e70jz"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfAu5LbPFT-f"
      },
      "source": [
        "# Build a neural network\n",
        "\n",
        "- Neural networks comprise of layers/modules that perform operations on data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your own neural netowrk. Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily. \n",
        "\n",
        "- In the following sections, we will build a neural network to classify images in the FashionMNIST dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS7-CIyCFTap"
      },
      "source": [
        "import os\n",
        "import torch \n",
        "from torch import nn \n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFCgtBwhHDOj"
      },
      "source": [
        "## Get a hardware device for training \n",
        "- We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let's check to see if [torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we continue to use the CPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tMZuPPrG6yV",
        "outputId": "1ca04d07-f933-4ad6-c5f8-5d2001cecfd5"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGreeVN5x4m8"
      },
      "source": [
        "## Define the class\n",
        "- We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKU6HSt2HhkX"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9LjArZz6ZpD"
      },
      "source": [
        "- We create an instance of `NeuralNetwork`, and move it to the `device`, and print it's structure. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feZ2wkaM6X4C",
        "outputId": "b905b16a-4561-4f37-ca84-67e4c562f263"
      },
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWbL5n26wCk"
      },
      "source": [
        "- To use the model, we pass it the input data. This executes the model's `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly! \n",
        "\n",
        "- Calling the model on the input returns a 10-dimentional tensor with raw predicted values for each class. We get the prediction densities by passing it through an instance of the `nn.Softmax` module. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnHtAnOY6mA2",
        "outputId": "56bb7f4d-2a37-4cba-db5b-d6c0f772a55c"
      },
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class: tensor([9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k7s5Z7pp7ca"
      },
      "source": [
        "## Model Layers\n",
        "- Let's break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size **28x28** and see what happens to it as we pass it through the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3mW-TrE74SX",
        "outputId": "1ec9bccc-3fb1-4832-8130-1e6dcb46f4d9"
      },
      "source": [
        "input_image = torch.rand(3, 28, 28)\n",
        "print(input_image.size())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi00LofPqxSQ"
      },
      "source": [
        "### nn.Flatten\n",
        "\n",
        "- We initialize the [`nn.flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGZJZSeqqkfh",
        "outputId": "3d2c9dfc-0ab2-4dbe-d003-a7c757b24e15"
      },
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyF8Bs0sr1jK"
      },
      "source": [
        "### nn.Linear\n",
        "\n",
        "- The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using it's stored weights and biases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTxErbgCrvrg",
        "outputId": "8b5223db-1c9a-4996-dc2e-2761762564f1"
      },
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l3GVIQ6soBm"
      },
      "source": [
        "### nn.ReLU\n",
        "- Non-linear activations are what create the complex mappings between the model's input and outputs. They are applied after linear transformations to introduce *nonlinearity*, helping neural networks learn a wide variety of phenomena. \n",
        "\n",
        "- In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there are other activations to introduce non-linearity in your model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2yIB6hbsd61",
        "outputId": "ff937e68-5fdc-4181-e74e-cdacf04bf824"
      },
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before ReLU: tensor([[-0.0025, -0.0661,  0.1621, -0.0492, -0.1363,  0.0035,  0.5103,  0.0418,\n",
            "         -0.2492, -0.2430,  0.1269,  0.0299,  0.3455,  0.9130,  0.4395, -0.4852,\n",
            "          0.1291, -0.4451,  0.1850,  0.3254],\n",
            "        [ 0.0589,  0.0508,  0.1624, -0.2068, -0.3638, -0.3798,  0.5338,  0.0761,\n",
            "         -0.2115, -0.2211,  0.0098,  0.1858,  0.4838,  0.3601,  0.3667, -0.5387,\n",
            "          0.1836, -0.2116,  0.1000,  0.2927],\n",
            "        [-0.1240, -0.1210,  0.1700, -0.1004, -0.2123, -0.0030,  0.8862,  0.0336,\n",
            "         -0.3050,  0.0255,  0.0917,  0.2158,  0.2024,  0.4596,  0.5321, -0.5373,\n",
            "         -0.1054, -0.2258,  0.3523,  0.1530]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0000, 0.0000, 0.1621, 0.0000, 0.0000, 0.0035, 0.5103, 0.0418, 0.0000,\n",
            "         0.0000, 0.1269, 0.0299, 0.3455, 0.9130, 0.4395, 0.0000, 0.1291, 0.0000,\n",
            "         0.1850, 0.3254],\n",
            "        [0.0589, 0.0508, 0.1624, 0.0000, 0.0000, 0.0000, 0.5338, 0.0761, 0.0000,\n",
            "         0.0000, 0.0098, 0.1858, 0.4838, 0.3601, 0.3667, 0.0000, 0.1836, 0.0000,\n",
            "         0.1000, 0.2927],\n",
            "        [0.0000, 0.0000, 0.1700, 0.0000, 0.0000, 0.0000, 0.8862, 0.0336, 0.0000,\n",
            "         0.0255, 0.0917, 0.2158, 0.2024, 0.4596, 0.5321, 0.0000, 0.0000, 0.0000,\n",
            "         0.3523, 0.1530]], grad_fn=<ReluBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smT-RBa-uam4"
      },
      "source": [
        "### nn.Sequential\n",
        "- [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl032adVuOxu"
      },
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3, 28, 28)\n",
        "logits = seq_modules(input_image)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQVn6g8qvdoe"
      },
      "source": [
        "###nn.Softmax\n",
        "- The last linear layer of the neural network returns `logits` - raw values in [`-infty`, `infty`] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values \\[0, 1] representing the model's predicted densities for each class. `dim` parameter indicates the dimension along which the values must sum to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq73G5LuvOqk"
      },
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZdkRCceys4k"
      },
      "source": [
        "## Model parameters\n",
        "- Many layers inside a neural network are *parameterized*, i.e. have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model's `parameters()` or `named_parameters()` methods. \n",
        "- In this example, we iterate over each parameter, and print its size and a preview of its values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqc2P82DwcMe",
        "outputId": "944d544f-ff33-4be2-8c09-d6488e891222"
      },
      "source": [
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  print(f\"Layer: {name} | Size: {param.size()} | Value : {param[:2]} \\n\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Value : tensor([[-0.0262, -0.0337,  0.0037,  ...,  0.0062,  0.0076,  0.0240],\n",
            "        [ 0.0035, -0.0225,  0.0264,  ..., -0.0203,  0.0158, -0.0293]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Value : tensor([ 0.0112, -0.0098], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Value : tensor([[-0.0056,  0.0028,  0.0233,  ..., -0.0189, -0.0012,  0.0063],\n",
            "        [ 0.0016, -0.0133, -0.0316,  ...,  0.0357, -0.0053,  0.0197]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Value : tensor([-0.0018,  0.0350], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Value : tensor([[ 0.0387, -0.0322, -0.0280,  ...,  0.0124, -0.0248,  0.0138],\n",
            "        [ 0.0247,  0.0179,  0.0058,  ..., -0.0041,  0.0107,  0.0111]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Value : tensor([ 0.0315, -0.0268], grad_fn=<SliceBackward>) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLeb-h-B0b6D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}