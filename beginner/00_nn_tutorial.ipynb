{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_nn_tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFeX2eYmk0jJXcxtSVTetB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desaiankitb/pytorch-basics/blob/main/beginner/00_nn_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBrT89e0BgYQ"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucel-DojBw2n"
      },
      "source": [
        "# What is `torch.nn` really?\n",
        "PyTorch provides the elegantly designed modules and classes [`torch.nn`](https://pytorch.org/docs/stable/nn.html), [`torch.optim`](https://pytorch.org/docs/stable/optim.html), [`Dataset`](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset), and [`DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) to help you create and train neural networks. In order to fully utilize their power and customize them for your problem, you need to really understand exactly what they are doing. To develop this understanding, we first train basic neural net on the MNIST data set without using any features from these models; we will initially only use the most basic PyTorch tensor functionality. Then, we will incrementally add one feature from `torch.nn`, `torch.optim`, `Dataset`, or `DataLoader` at a time, showing exactly what each piece does, and how it works to make the code either more concise, or more flexible. \n",
        "\n",
        "## MNIST datasetup \n",
        "We will use the classic [`MNIST`](http://deeplearning.net/data/mnist/) dataset, which consists of black-and-white images of hand-drawn digits (between 0 and 9). \n",
        "\n",
        "We will use [`pathlib`](https://docs.python.org/3/library/pathlib.html) for dealing with paths (part of the Python 3 standard lib), and will download the dataset using [`requests`](http://docs.python-requests.org/en/master/). We will only import modules when we use them, so you can see exactly what is being used at each point. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-BH9PmcBwF_"
      },
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "  content = requests.get(URL + FILENAME).content\n",
        "  (PATH / FILENAME).open(\"wb\").write(content)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6SMy67iEvcP"
      },
      "source": [
        "This dataset is in numpy array format, and has been stored using pickle, a python-specific format for serializing data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lduiuuYjEtj-"
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "  ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jncl3ZdFSmi"
      },
      "source": [
        "Each image is 28 x 28, and is being stored as a flattened row of length 784 (=28x28). Let us take a look at one; we need to reshape it to 2D first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "5n9QQ8XCFQaW",
        "outputId": "aa151dcd-ab0b-4de6-89b5-3f68f7c9955a"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy as np \n",
        "\n",
        "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
        "print(x_train.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHwNDcxIFyKK"
      },
      "source": [
        "PyTorch uses `torch.tensor`, rather than numpy arrays, so we need to convert our data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JtuDuRIFvvG",
        "outputId": "40c82970-835e-49e2-a18a-927e483dfa37"
      },
      "source": [
        "import torch \n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        ")\n",
        "n, c = x_train.shape\n",
        "print(x_train, y_train)\n",
        "print(x_train.shape)\n",
        "print(y_train.min(), y_train.max())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
            "torch.Size([50000, 784])\n",
            "tensor(0) tensor(9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrYQTIB6a3-Q"
      },
      "source": [
        "## Neural network from scratch (no `torch.nn`)\n",
        "\n",
        "Let us first create a model using nothing but PyTorch tensor operations. We are assuming you are already familiar with the basics of neural networks. (If you are not, you can learn them at [`course.fast.ai`](https://course.fast.ai)\n",
        "\n",
        "PyTorch provides mothods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model. These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation *automatically*! \n",
        "\n",
        "For the weights, we set `requires_grad` **after** the initialization, since we don't want that step included in the gradient. (Note that a trailing `_` in PyTorch signifies that the operations is performed in-place.) \n",
        "\n",
        "> **Note:** we are initializing the weights here with [`Xavier initialisation` ](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) by multiplying with $\\frac{1}{\\sqrt(n)}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JXgv-73GK-r"
      },
      "source": [
        "import math \n",
        "\n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCCLN8DFjuvh"
      },
      "source": [
        "Thanks to PyTorch's ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model! So let us just write a plain matrix multiplication and broadcasted addition to create a simple linear model. We also need an activation function, so we will write *log_softmax* and use it. Remember: although PyTorch provides a lot of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python. PyTorch will even create fast GPU or vectorized CPU code for your function automatically. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoaHl-xZjtwp"
      },
      "source": [
        "def log_softmax(x):\n",
        "  return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "\n",
        "def model(xb):\n",
        "  return log_softmax(xb @ weights + bias)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no_UXwwzk4KI"
      },
      "source": [
        "In the above, the `@` stands for dot product operation. We will call our function on one batch of data (in this case, 64 images). This is one *forward pass*. Note that our predictions would not be any better than random at this stage, since we start with random weights.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPALKPwKk1GP",
        "outputId": "a4ef7cff-a5a2-4bfc-fb6f-54a7b3d92006"
      },
      "source": [
        "bs = 64 # batch size \n",
        "\n",
        "xb = x_train[0:bs] # a mini-batch from x \n",
        "preds = model(xb) # predictions \n",
        "preds[0], preds.shape\n",
        "print(preds[0], preds.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-2.4529, -2.8615, -1.7410, -2.2808, -2.3760, -2.5526, -2.1136, -2.2545,\n",
            "        -2.3209, -2.4709], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0QlGBv7lnnw"
      },
      "source": [
        "As you see, the `preds` tensor contains not only the tensor values, but also a gradient function. We will use this later to do backprop. \n",
        "\n",
        "Let us implement negative log-likelihood to use as the loss function (again, we can just use standard Python):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3-11oxKljCR"
      },
      "source": [
        "def nll(input, target):\n",
        "  return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = nll"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4KMH3X7mK45"
      },
      "source": [
        "let us check our loss with our random model, so we can see if we improve after a backprop pass later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUDOZQ9xmKEB",
        "outputId": "bcc2ee3f-6702-49b0-fc89-43fa48060a2e"
      },
      "source": [
        "yb = y_train[0:bs]\n",
        "print(loss_func(preds, yb))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3610, grad_fn=<NegBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr-VuhmHmbWL"
      },
      "source": [
        "Let us also implement a function to calculate the accuracy of our model. For each prediction, if the index with the largest value matches the target value, then the prediction was correct. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUQf_sxNmXTt"
      },
      "source": [
        "def accuracy(out, yb):\n",
        "  preds = torch.argmax(out, dim=1)\n",
        "  return (preds == yb).float().mean()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaO-zrvemtyL"
      },
      "source": [
        "Let us check the accuracy of our random model, so we can see if our accuracy improves as our loss improves. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smdhYa4-ms8z",
        "outputId": "4883d73f-3303-4156-f42d-dcbfb693f40a"
      },
      "source": [
        "print(accuracy(preds, yb))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0938)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS2C8UT4m-3G"
      },
      "source": [
        "We can now run a loop. For each iteration, we will: \n",
        "* select a mini-batch of data (of size `bs`)\n",
        "* use the model to make predictions \n",
        "* calculate the loss \n",
        "* `loss.backward()` updates the gradients of the model, in this case, `weights` and `bias`. \n",
        "\n",
        "We now use these gradients to update the weights and bias. We do this within the `torch.no_grad()` context manager, because we do not want these actions to be recorded for our next calculation of the gradient. You can read more about how PyTorch's Autograd records operations [here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
        "\n",
        "We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. `loss.backward()` *adds* gradients to whatever it already stored, rather than replacing them). \n",
        "\n",
        "> **Tip:** You can use the standard python debugger to step through PyTorch code, allowing you to check the various variable values at each step. Uncomment `set_trace()` below to try it out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "So8WrdYWqiMo",
        "outputId": "c53f8516-ba15-4c7c-9bbb-d07f97e3764d"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.2.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.25.0-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.0.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.19-py3-none-any.whl (368 kB)\n",
            "\u001b[K     |████████████████████████████████| 368 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=85b9bf726fafd6fdbb967c41f2de996463a094183c5900eee2b51d6c06cae2e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.19 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.25.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.25.0 prompt-toolkit-3.0.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8gZFu5im7su"
      },
      "source": [
        "# from IPython.core.debugger import set_track\n",
        "\n",
        "lr = 0.5 # learning rate \n",
        "epochs = 2 # how many epochs to train for \n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range((n-1) // bs + 1):\n",
        "    # set_trace()\n",
        "    start_i = i * bs\n",
        "    end_i = start_i + bs\n",
        "    xb = x_train[start_i:end_i]\n",
        "    yb = y_train[start_i:end_i]\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "      weights -= weights.grad * lr\n",
        "      bias -= bias.grad * lr \n",
        "      weights.grad.zero_()\n",
        "      bias.grad.zero_()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc6-N2uyq4tk"
      },
      "source": [
        "That is it: we have created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch! \n",
        "\n",
        "Let us check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have descreased and accuracy to have increased, and they have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD9uA381qUkc",
        "outputId": "cd70d264-347b-4839-fc7b-d83cf578d495"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb),yb))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0809, grad_fn=<NegBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FcYyeP-rbvN"
      },
      "source": [
        "## Using `torch.nn.functional` \n",
        "We will now refactor our code, so that it does the same thing as before, only we will start taking advantage of PyTorch's `nn` classes to make it more concise and flexible. At each step from here, we should be making our code one or more of: shorter, more understandable, and/or more flexible. \n",
        "\n",
        "The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from `torch.nn.functional` (which is generally imported into the namespace `F` by convention). This module contains all the functions in the `torch.nn` library (whereas other parts of the library contain classes). As well as a wide range of loss and activation functions, you wil also find here some convenient functions for creating neural nets, such as pooling functions. (There are also functions for doing convolutions, linear layers, etc, but as we will see, these are usually better handled using other parts of the library). \n",
        "\n",
        "If you are using negative log likelihood loss and log softmax activation, then PyTorch provides single function `F.cross_entropy` that combines the two. So we can even remove the activation function from our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuunCH2J2tuJ"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "  return xb @ weights + bias "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3McNgW9AXQh"
      },
      "source": [
        "Note that we no longer call `log_softmax` in the `model` function. Let us confirm that our loss and accuracy are the same as before: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8onFFCPA3LDJ",
        "outputId": "fb17b0b3-c5a0-4319-af14-399b63ef9da3"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0809, grad_fn=<NllLossBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjFoDy_nCb4c"
      },
      "source": [
        "## Refactor using `nn.Module`\n",
        "Next up, we will use `nn.Module` and `nn.Parameter`, for a clearer and more concise training loop. We subclass `nn.Module` (which itself is a class and able to keep track of state). In this case, we want to create a class that holds our weights, bias, and method for the forward step. `nn.Module` has a number of attributes and methods (such as `.parameters()` and `.zero_grad()`) which we will be using. \n",
        "\n",
        "> **NOTE:** `nn.Module` (uppercase M) is a PyTorch specific concept, and is a class we will be using a lot. `nn.Module` is not to be confused with the Pythong concept of a (lowercase `m`) [module](https://docs.python.org/3/tutorial/modules.html), which is a file of Python code that can be imported. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fPfQP7DAqQ6"
      },
      "source": [
        "from torch import nn \n",
        "\n",
        "class Mnist_Logistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
        "    self.bias = nn.Parameter(torch.zeros(10))\n",
        "\n",
        "  def forward(self, xb):\n",
        "    return xb @ self.weights + self.bias "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfCaerMbE5ks"
      },
      "source": [
        "Since we are now using an object instead of just using function, we first have to instantiate our model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFJ3CnFnE5AS"
      },
      "source": [
        "model = Mnist_Logistic()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRIPASFyFHEg"
      },
      "source": [
        "Now we can calculate the loss in the same way as before. Note that `nn.Module` objects are used as if they are functions (i.e. they are *callable*), but behind the scenes PyTorch will call our `forward` method automatically. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMAM4mYBFF89",
        "outputId": "cc6b8b9d-75ea-4296-dc95-0841f4ae3b3f"
      },
      "source": [
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3347, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyBdPtEfF7-Z"
      },
      "source": [
        "Previously for our training loop we had to update the values for each parameter by name, and manually zero out the grads for each parameter separately, like this: \n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "  weights -= weights.grad * lr\n",
        "  bias -= bias.grad * lr \n",
        "  weights.grad.zero_()\n",
        "  bias.grad.zero_()\n",
        "```\n",
        "\n",
        "Now we can take advantage of `model.parameters()` and `model.zero_grad()` (which are both defined by PyTorch for `nn.Module`) to make those steps more concise and less prone to the error for getting some of our parameters, particularly if we had a more complicated model: \n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "  for p in model.parameters(): p -= p.grad * lr \n",
        "  model.zero_grad()\n",
        "```\n",
        "\n",
        "we will wrap our little training loop in a `fit` function so we can run in again later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm_c5QN_Fxy9"
      },
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range((n - 1) // bs + 1):\n",
        "            start_i = i * bs\n",
        "            end_i = start_i + bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters():\n",
        "                    p -= p.grad * lr\n",
        "                model.zero_grad()\n",
        "\n",
        "fit()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBp1X_eXJQub"
      },
      "source": [
        "Let us double-check that our loss has gone down:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls-If0IzHpSG",
        "outputId": "aff02978-f801-479f-d515-53182272d8a8"
      },
      "source": [
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0817, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcKB856w1MCw"
      },
      "source": [
        "# Refactor using `nn.Linear`\n",
        "We continue to refactor our code. Instead of manually defining and initializing `self.weights` and `self.bias`, and calculating `xb @ self.weights + self.bias`, we will instead use the PyTorch class [`nn.Linear`](https://pytorch.org/docs/stable/nn.html#linear-layers) for a linear layer, which does all that for us. PyTorch has many types of predefined layers that can grately simplify our code, and often makes it faster too. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxAIW_CPJYk8"
      },
      "source": [
        "class Mnist_Logistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lin = nn.Linear(784, 10)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    return self.lin(xb)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBtKvUT52HPx"
      },
      "source": [
        "We instantiate our model and calculate the loss in the same way as before: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCe-BozT2M5f",
        "outputId": "f2712a7f-d3f7-4b4e-8c73-79de12b9814f"
      },
      "source": [
        "model = Mnist_Logistic()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3502, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgoncx0a2X9B"
      },
      "source": [
        "We are still able to use our same `fit` method as before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryxVLCQw2Ux7",
        "outputId": "63fa6757-794c-4727-f5af-67275f546066"
      },
      "source": [
        "fit()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0816, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiwQ-uoY4cjT"
      },
      "source": [
        "## Refactor using optim \n",
        "PyTorch also has a package with various optimization algorithms, `torch.optim`. We can use the `step` method from our optimizer to take a forward step, instead of manually updating each parameter. \n",
        "\n",
        "This will let us replace our previous manually coded optimization step: \n",
        "```\n",
        "with torch.no_grad():\n",
        "  for p in model.parameters(): \n",
        "    p -= p.grad * lr\n",
        "    model.zero_grad()\n",
        "```\n",
        "\n",
        "and instead use just: \n",
        "\n",
        "```\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "```\n",
        "\n",
        "(`optim.zero_grad()` resets the gradient to 0 and we need to call it before computing the gradient for the next minibatch.) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTINq2n2gKB"
      },
      "source": [
        "from torch import optim"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Q5y5tR85F0"
      },
      "source": [
        "We will define a little function to create our model and optimizer so we can reuse it in the future. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTwxxu4F83jh",
        "outputId": "bc7bbf8b-bded-4cdc-b4f9-7f2550e943a0"
      },
      "source": [
        "def get_model():\n",
        "  model = Mnist_Logistic()\n",
        "  return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "model, opt = get_model()\n",
        "print(loss_func(model(xb), yb))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range(( n - 1) // bs + 1):\n",
        "    start_i = i * bs\n",
        "    end_i = start_i + bs \n",
        "    xb = x_train[start_i:end_i]\n",
        "    yb = y_train[start_i:end_i]\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3069, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0817, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYH-17IN_JXV"
      },
      "source": [
        "## Refactor using Dataset\n",
        "PyTorch has an abstract Dataset class. A Dataset can be anything that has a `__len__` function (called by Python's standard `len` function) and a `__getitem__` function as a way of indexing into it. [This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) walks through a nice example of creating a custom `FacialLandmarkDataset` class as a subclass of `Dataset`. \n",
        "\n",
        "PyTorch's [`TensorDataset`](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) is a Dataset wrapping tensors. By defining a length and way of indexing, this algo gives us a way to iterate, index, and slice along the first dimention of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7rNonfJ-Lmu"
      },
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izdAjf5lCbA0"
      },
      "source": [
        "Both `x_train` and `y_train` can be combined in a single `TensorDataset`, which will be easier to iterate over and slice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xJDyEz0Caav"
      },
      "source": [
        "train_ds = TensorDataset(x_train, y_train)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3rt4dkoCp5P"
      },
      "source": [
        "Previously, we had to iterate through minibatches of x and y separately: \n",
        "\n",
        "```\n",
        "xb = x_train[start_i:end_i]\n",
        "yb = y_train[start_i:end_i]\n",
        "```\n",
        "\n",
        "Now, we can do these two steps together: \n",
        "\n",
        "```\n",
        "xb, yb = train_ds[ibs: ibs+bs]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNqFfnX3CpSl",
        "outputId": "633ec71b-25e6-4416-a494-d5e0168f3e2f"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "    xb, yb = train_ds[i * bs: bs + bs]\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(nan, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0e1BrEYDt2F"
      },
      "source": [
        "# Refactor using DataLoader\n",
        "\n",
        "PyTorch's `DataLoader` is responsible for managing batches. You can create a `DataLoader` from `Dataset`. `DataLoader` makes it easier to iterate over batches. Rather than just having to use `train_ds[i*bs : i*bs+bs]` the `DataLoader` gives us each minibatch automatically. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QBgISnODiUe"
      },
      "source": [
        "from torch.utils.data import DataLoader \n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZQEbIJfEdz1"
      },
      "source": [
        "Previously, our loop iterated over batches (xb, yb) like this: \n",
        "\n",
        "```\n",
        "for i in range((n-1)//bs+1):\n",
        "  xb, yb = train[ibs:ibs+bs]\n",
        "  pred = model(xb)\n",
        "```\n",
        "\n",
        "Now, our loop is much cleaner, as `(xb, yb)` are loaded automatically from the `DataLoader`:: \n",
        "\n",
        "```\n",
        "for xb, yb in train_dl:\n",
        "  pred = model(xb)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7332p7DQEda_",
        "outputId": "761a6ad9-ddeb-4f71-ef0b-41162e0a2faf"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for xb, yb in train_dl:\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0820, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoKcaYgAFTnb"
      },
      "source": [
        "Thanks to PyTorch's `nn.Module`, `nn.Parameter`, `Dataset` and `DataLoader`, our training loop is now dramatically smaller and easier to understand. Let us now try to add basic features necessary to create effective models in practice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jACiDAZ7FRpk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}