{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_polynomial_optim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUNzEBASz310iMKxynVnI1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desaiankitb/pytorch-basics/blob/main/pytorch-with-examples/05_polynomial_optim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQhFGDhf5xb4"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDb8LJB75-Y5"
      },
      "source": [
        "PyTorch: optim\n",
        "--------------\n",
        "\n",
        "A third order polynomial, trained to predict $y=\\sin(x)$ from $-\\pi$\n",
        "to $\\pi$ by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses the nn package from PyTorch to build the network.\n",
        "\n",
        "Rather than manually updating the weights of the model as we have been doing,\n",
        "we use the optim package to define an Optimizer that will update the weights\n",
        "for us. The optim package defines many optimization algorithms that are commonly\n",
        "used for deep learning, including SGD+momentum, RMSProp, Adam, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fZyk45H51Lg",
        "outputId": "8486b214-e13b-48b0-fc5b-781c9343b56b"
      },
      "source": [
        "import torch\n",
        "import math \n",
        "\n",
        "# Create Tensors to hold input and outputs. \n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Prepare the input tensor (x, x^2, x^3)\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "# Usage the nn package to define our model and loss function. \n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "# Use the optim package to define an Optimizer that will update the weights of \n",
        "# the model for us. Here, we will use RMSprop; the optim package contains many other \n",
        "# optimization algorithms. The first argument to the RMSprop constructor tells the \n",
        "# optimizer which Tensor it should update. \n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(2000):\n",
        "  # Forward pass: compute predicted y by passing x to the model. \n",
        "  y_pred = model(xx)\n",
        "\n",
        "  # Compute the print loss. \n",
        "  loss = loss_fn(y_pred, y)\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss.item())\n",
        "\n",
        "  # Before the backward pass, use the optimizer object to zero all of the \n",
        "  # gradients for the variables it will update (which are the learnable\n",
        "  # weights of the model). This is because by default, gradients are \n",
        "  # accumulated in buffers (i.e. not overwritten) whenever .backward()\n",
        "  # is called. Checkout docs of torch.autograd.backward for more details. \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Backward pass: compute gradient of the loss with respect to model \n",
        "  # parameters \n",
        "  loss.backward()\n",
        "\n",
        "  # Calling the step function on an Optimizer makes an update to its \n",
        "  # parameters \n",
        "  optimizer.step()\n",
        "\n",
        "linear_layer = model[0]\n",
        "print(f'Result: y = ' + \n",
        "        f'{\" %.4f +\"}'%linear_layer.bias.item()  + \n",
        "        f'{\" %.4f x +\"}'%linear_layer.weight[:, 0].item() + \n",
        "        f'{\" %.4f x^2 +\"}'%linear_layer.weight[:, 1].item() + \n",
        "        f'{\" %.4f x^3 +\"}'%linear_layer.weight[:, 2].item() \n",
        "        )\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 3875.854248046875\n",
            "199 1880.682861328125\n",
            "299 902.89111328125\n",
            "399 469.6977844238281\n",
            "499 290.47772216796875\n",
            "599 188.77310180664062\n",
            "699 118.66969299316406\n",
            "799 70.68668365478516\n",
            "899 39.44465637207031\n",
            "999 21.14144515991211\n",
            "1099 12.35662841796875\n",
            "1199 9.44282054901123\n",
            "1299 8.939781188964844\n",
            "1399 8.903680801391602\n",
            "1499 8.905030250549316\n",
            "1599 8.913844108581543\n",
            "1699 8.908075332641602\n",
            "1799 8.9666109085083\n",
            "1899 8.943113327026367\n",
            "1999 8.92124080657959\n",
            "Result: y =  -0.0005 + 0.8572 x + -0.0005 x^2 + -0.0928 x^3 +\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU_c3XW4iELb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}